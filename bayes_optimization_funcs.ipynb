{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Синтетический датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation score: 0.85\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "print(f\"Cross validation score: {np.mean(cross_val_score(tree, X_test, y_test))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оптимизация\n",
    "\n",
    "[Black-Box Optimization Challenge, или как подбирать гиперпараметры для моделей](https://habr.com/ru/companies/hsespb/articles/537020/) (Дата обращения 29.03.2025)\n",
    "\n",
    "План: \n",
    "- [x] Строим гауссовский процесс по уже имеющимся наблюдениям $\\{x_i, y_i\\}$.\n",
    "- [ ] Сэмплируем какое-то количество кандидатов $n_{cand}$ (в нашем решении $n_{cand}=min(100*D, 5000)$, где $D$ — количество оптимизируемых параметров).\n",
    "- [ ] Используя построенный гауссовский процесс, мы вычисляем оценки на средние и дисперсии значений в кандидатах.\n",
    "- [ ] Вычисляем функцию приобретения в этих $n_{cand}$ кандидатах и выбираем 8 с наибольшими значениями.\n",
    "- [ ] Возвращаем эти точки как наши предложения проверяющему коду.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гауссовский процесс\n",
    "\n",
    "- [Регрессия гауссовского процесса с самого начала](https://habr.com/ru/companies/skillfactory/articles/562892/) (Дата обращения 29.03.2025)\n",
    "- [\\[DeepBayes\\] День 4, лекция 2. Гауссовские процессы и байесовская оптимизация](https://youtu.be/PgJMLpIfIc8?si=Xkxg0Ndqox6NNqUp) (Дата обращения 29.03.2025)\n",
    "\n",
    "\n",
    "Дано:\n",
    "$$\n",
    "\\begin{align}\n",
    "    & \\mathbf{X} = \\{x_1, ..., x_n\\} \\\\\n",
    "    & \\mathbf{f} = \\{f_{1}(x_1), ..., f_{n}(x_n)\\} = \\mathbf{y} = \\{y_1, ..., y_n\\} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Обучение:\n",
    "$$\n",
    "\\begin{align}\n",
    "    & p(\\mathbf{f}|\\mathbf{X}) = \\mathcal{N}(\\mu, \\mathbf{K}) \\\\\n",
    "    & f(\\mathbf{x}) \\sim \\mathcal{GP}(\\mu(\\mathbf{x}), k(\\mathbf{x}, \\mathbf{x}^{\\prime})) \\\\\n",
    "    & \\mu(\\mathbf{x}) = \\mathbb{E}(f(\\mathbf{x})) \\\\\n",
    "    & k(\\mathbf{x}, \\mathbf{x}^{\\prime}) = \\mathbb{E}(f(\\mathbf{x} - m(\\mathbf{x}))f(\\mathbf{x}^{\\prime} - m(\\mathbf{x}^{\\prime}))) = cov(\\mathbf{x}, \\mathbf{x}^{\\prime})=\\sigma_{f}^{2}\\exp\\{-\\sum_{i=1}^{d}\\frac{(x_{i}-x_{i}^{\\prime})^2}{2r_{i}^2}\\}\\text{ - rbf kernel}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Предсказание:\n",
    "$$\n",
    "\\begin{align}\n",
    "    & y_* = f_{*}(\\mathbf{x}_{*}) + \\epsilon_{*} \\\\\n",
    "    & p(\\mathbf{y}, f_{*}) = \\mathcal{N}\\left(0, \\left[ \\begin{matrix} \n",
    "        \\mathbf{K} + \\sigma^2 I_m & \\mathbf{k}_* \\\\ \n",
    "        \\mathbf{k}_*^{\\top} & \\mathbf{K}_{**} \n",
    "    \\end{matrix} \\right]\\right) \\\\\n",
    "    & \\mu_\\ast = \\mathbf{k}_\\ast^\\mathrm{T} [\\mathbf{K} + \\sigma^2 \\mathbf{I}_m]^{-1} \\mathbf{y}, \\\\\n",
    "    & \\sigma_\\ast^2 = \\bar{K}_{\\ast\\ast} - \\mathbf{k}_\\ast^\\mathrm{T} [\\mathbf{K} + \\sigma^2 \\mathbf{I}_m]^{-1} \\mathbf{k}_\\ast\n",
    "\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    def __init__(self, sigma=1, r=1, noise=0.1):\n",
    "        self.sigma = sigma # Предполагаемое распределение\n",
    "        self.r = r # Данный параметр масштабирует значения ковариационной функции\n",
    "        self.noise = noise # Ввожу шум для борьбы с вырожденными ковариационными матрицами \n",
    "\n",
    "    def rbf_kernel(self, xi, xj, sigma=1.0, r=1.0):\n",
    "        return sigma**2 * np.exp( -np.sum( (xi - xj)**2 / (2 * r**2) ) ) # Абсолютно гладкое гауссовское ядро\n",
    "    \n",
    "    def cov(self, X1, X2=None):\n",
    "        if X2 is None:\n",
    "            X2 = X1\n",
    "        return np.array(\n",
    "            [self.rbf_kernel(x1, x2, self.sigma, self.r) for x1 in X1 for x2 in X2]\n",
    "        ).reshape( (len(X1), len(X2)) )\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = np.array(X_train)\n",
    "        self.y_train = np.array(y_train) \n",
    "        return self\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = np.array(X_test)\n",
    "        K = self.cov(self.X_train) # Считаем ковариационные матрицы трейна с самим собой\n",
    "        K_ss = self.cov(X_test) # Считаем ковариационные матрицы теста с самим собой\n",
    "        K_s = self.cov(self.X_train, X_test) # Считаем ковариационные матрицы трейна и тестом\n",
    "        K_inv = np.linalg.inv(K + self.noise**2 * np.eye(len(self.X_train))) # Обратная матрица с регуляризационным членом\n",
    "        mu_s = K_s.T @ K_inv @ self.y_train # Предсказываем среднее\n",
    "        cov_s = K_ss - K_s.T @ K_inv @ K_s # Предсказываем дисперсии\n",
    "        return (mu_s, np.diag(cov_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997494920668502"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_GP, y_GP = make_regression(n_features=1)\n",
    "X_train_GP, X_test_GP, y_train_GP, y_test_GP = train_test_split(X_GP, y_GP, test_size=0.5)\n",
    "\n",
    "model_GP = GaussianProcess().fit(X_train_GP, y_train_GP)\n",
    "mu, var = model_GP.predict(X_test_GP)\n",
    "\n",
    "r2_score(y_test_GP, mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
